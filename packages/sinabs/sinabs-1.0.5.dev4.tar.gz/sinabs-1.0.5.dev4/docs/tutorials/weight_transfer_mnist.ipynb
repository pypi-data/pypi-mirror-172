{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting an ANN to an SNN\n",
    "\n",
    "This tutorial walks you through how to convert your pre-trained model to a spiking version.\n",
    "Lets start by installing all the necessary packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an ANN\n",
    "We define a simple convolutional architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "ann = nn.Sequential(\n",
    "    nn.Conv2d(1, 20, 5, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(20, 32, 5, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(32, 128, 3, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128, 500, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 10, bias=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to fine-tune our model on our dataset of choice. Here we'll use MNIST. Since we intend to do a spiking neural network simulation, we override this `Dataset` to also *optionally* return a `spike raster` instead of an image. \n",
    "\n",
    "In this implementation of the `Dataset` we use *rate coding* to generate a series of spikes at each pixel of the image proportional to it's gray level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class MNIST(datasets.MNIST):\n",
    "    def __init__(self, root, train=True, is_spiking=False, time_window=100):\n",
    "        super().__init__(\n",
    "            root=root, train=train, download=True, transform=transforms.ToTensor()\n",
    "        )\n",
    "        self.is_spiking = is_spiking\n",
    "        self.time_window = time_window\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index].unsqueeze(0) / 255, self.targets[index]\n",
    "        # img is now a tensor of 1x28x28\n",
    "\n",
    "        if self.is_spiking:\n",
    "            img = (torch.rand(self.time_window, *img.shape) < img).float()\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the ANN\n",
    "\n",
    "We'll make sure that classification accuracy is high enough with this model on MNIST. Note here that we are not yet using spiking input (`spiking=False`). This is vanilla training for standard image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist_train = MNIST(\"./data\", train=True, is_spiking=False)\n",
    "train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True)\n",
    "\n",
    "mnist_test = MNIST(\"./data\", train=False, is_spiking=False)\n",
    "test_loader = DataLoader(mnist_test, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over our data loader `train_loader` and train our parameters using the `Adam` optimizer with a learning rate of `1e-4`. Since the last layer in our network has no specific activation function defined, `cross_entropy` loss is a good candidate to train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e58747d18b4e3e80773d991845bc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ann = ann.to(device)\n",
    "ann.train()\n",
    "\n",
    "optim = torch.optim.Adam(ann.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "for n in tqdm(range(n_epochs)):\n",
    "    for data, target in iter(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = ann(data)\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 97.86%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = []\n",
    "\n",
    "for data, target in iter(test_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = ann(data)\n",
    "\n",
    "    # get the index of the max log-probability\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "    # Compute the total correct predictions\n",
    "    correct_predictions.append(pred.eq(target.view_as(pred)))\n",
    "\n",
    "correct_predictions = torch.cat(correct_predictions)\n",
    "print(\n",
    "    f\"Classification accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this model on `MNIST` is fairly straight forward and you should reach accuracies of around `>98%` within a small number of epochs. In the script above we only train for 3 epochs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model conversion to SNN\n",
    "\n",
    "Up until this point we have only operated on images using standard CNN architectures. Now we look at how to build an equivalent spiking convolutional neural network (`SCNN`).\n",
    "\n",
    "`sinabs` has a handy method for this. Given a standard CNN model, the `from_model` method in `sinabs` that converts it into a spiking neural network. It is a *one liner*! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinabs.from_torch import from_model\n",
    "\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "sinabs_model = from_model(\n",
    "    ann, input_shape=input_shape, add_spiking_output=True, synops=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this method takes two more parameters in addition to the model to be converted.\n",
    "\n",
    "`input_shape` is needed in order to instantiate a SNN with the appropriate number of neurons because unlike traditional CNNs, SNNs are *stateful*.\n",
    "\n",
    "`add_spiking_output` is a boolean flag to specify whether or not to add a spiking layer as the last layer in the network. This ensure that both the input and output to our network are of the form of `spikes`.\n",
    "\n",
    "`synops=True` tells sinabs to include the machinery for calculating synaptic operations, which we'll use later.\n",
    "\n",
    "Let us now look at the generated SCNN. You should see that the only major difference is that the `ReLU` layers are replaced by `SpikingLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "  (1): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0)\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "  (4): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0)\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (7): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0)\n",
       "  (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (9): Flatten(start_dim=1, end_dim=-1)\n",
       "  (10): Linear(in_features=128, out_features=500, bias=False)\n",
       "  (11): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0)\n",
       "  (12): Linear(in_features=500, out_features=10, bias=False)\n",
       "  (Spiking output): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinabs_model.spiking_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation in sinabs simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our SCNN model to verify whether the network is in fact \"equivalent\" to the CNN model in terms of its performance. As we did previously, we start by defining a data loader (this time it is going to produce spikes, `spiking=True`) and then pass it to our test method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window per sample\n",
    "time_window = 100  # time steps\n",
    "test_batch_size = 10\n",
    "\n",
    "spike_mnist_test = MNIST(\n",
    "    \"./data\", train=False, is_spiking=True, time_window=time_window\n",
    ")\n",
    "spike_test_loader = DataLoader(\n",
    "    spike_mnist_test, batch_size=test_batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the spiking simulations are significantly slower on a PC, we are going to limit our test to 300 samples here. You can of course test it on the entire 10k samples if you want to verify that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83547d1c20d44144bfeb6c21742b998f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 98.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "import sinabs.layers as sl\n",
    "\n",
    "correct_predictions = []\n",
    "\n",
    "for data, target in tqdm(spike_test_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    data = sl.FlattenTime()(data)\n",
    "    with torch.no_grad():\n",
    "        output = sinabs_model(data)\n",
    "        output = output.unflatten(\n",
    "            0, (test_batch_size, output.shape[0] // test_batch_size)\n",
    "        )\n",
    "\n",
    "    # get the index of the max log-probability\n",
    "    pred = output.sum(1).argmax(dim=1, keepdim=True)\n",
    "\n",
    "    # Compute the total correct predictions\n",
    "    correct_predictions.append(pred.eq(target.view_as(pred)))\n",
    "    if len(correct_predictions) * test_batch_size >= 300:\n",
    "        break\n",
    "\n",
    "correct_predictions = torch.cat(correct_predictions)\n",
    "print(\n",
    "    f\"Classification accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this auto-generated spiking (`sinabs_model`) network's performance is close to that of the `ann`! Yay!\n",
    "\n",
    "You would have noticed a free parameter that was added `time_window`. This is a critical parameter that determines whether or not your SNN is going to work well. The longer `time_window` is, the more spikes we produce as input and the better the performance of the network is going to be. Feel free to experiment with this parameter and see how this changes your network performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of specific example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one sample from the dataloader\n",
    "img, label = spike_mnist_test[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize this data, just so we know what to expect. We can do this by collapsing the time dimension of the spike raster returned by the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(img.sum(0)[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take this data (including the time dimension), and pass it to the Sinabs SNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_output = sinabs_model(img.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now display the output in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.pcolormesh(snn_output.T.detach().cpu())\n",
    "\n",
    "plt.ylabel(\"Neuron ID\")\n",
    "plt.yticks(np.arange(10) + 0.5, np.arange(10))\n",
    "plt.xlabel(\"Time\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the majority of spikes are emitted by the output neuron corresponding to the digit plotted above, which is a correct inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "caf264bf03997fa53b380c84044763293a7a6f8ebb5555ee5243fd4d1f495be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
