Metadata-Version: 2.1
Name: getdaft
Version: 0.0.16
Summary: A Distributed DataFrame library for large scale complex data processing.
Home-page: https://getdaft.io
License: Apache-2.0
Author: Eventual Inc
Author-email: daft@eventualcomputing.com
Maintainer: Sammy Sidhu
Maintainer-email: sammy@eventualcomputing.com
Requires-Python: >=3.7.1,<4.0.0
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Provides-Extra: experimental
Provides-Extra: iceberg
Provides-Extra: serving
Requires-Dist: Pillow (>=9.2.0,<10.0.0); extra == "experimental"
Requires-Dist: PyYAML (>=6.0,<7.0); extra == "serving" or extra == "experimental"
Requires-Dist: boto3 (>=1.23.0,<2.0.0); extra == "serving" or extra == "experimental"
Requires-Dist: cloudpickle (>=2.1.0,<3.0.0); extra == "serving" or extra == "experimental"
Requires-Dist: docker (>=5.0.3,<6.0.0); extra == "serving" or extra == "experimental"
Requires-Dist: fastapi (>=0.79.0,<0.80.0); extra == "serving" or extra == "experimental"
Requires-Dist: fsspec
Requires-Dist: icebridge (>=0.0.4,<0.0.5); extra == "iceberg" or extra == "experimental"
Requires-Dist: loguru (>=0.6.0,<0.7.0)
Requires-Dist: numpy (>=1.16.6,<2.0.0)
Requires-Dist: pandas (>=1.3.5,<2.0.0)
Requires-Dist: pickle5 (>=0.0.12,<0.0.13); python_version < "3.8"
Requires-Dist: polars[timezone] (>=0.14.12,<0.15.0)
Requires-Dist: protobuf (>=3.19.0,<3.20.0)
Requires-Dist: pyarrow (>=6,<7)
Requires-Dist: pydot (>=1.4.2,<2.0.0)
Requires-Dist: ray (==1.13.0)
Requires-Dist: tabulate (>=0.8.10,<0.9.0)
Requires-Dist: typing-extensions (>=4.0.0); python_version < "3.8"
Requires-Dist: uvicorn (>=0.18.2,<0.19.0); extra == "serving" or extra == "experimental"
Project-URL: Repository, https://github.com/Eventual-Inc/Daft
Description-Content-Type: text/markdown

[![daft](https://github.com/Eventual-Inc/Daft/actions/workflows/python-package.yml/badge.svg)](https://github.com/Eventual-Inc/Daft/actions/workflows/python-package.yml)

# Welcome to Daft

[Daft](https://www.getdaft.io) is a fast, ergonomic and scalable open-source dataframe library: built for Python and Complex Data/Machine Learning workloads.

> **Daft is currently in its Alpha release phase - please expect bugs and rapid improvements to the project.**
> **We welcome user feedback/feature requests in our [Discussions forums](https://github.com/Eventual-Inc/Daft/discussions).**

![Frame 113](https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png)


## Installation

Install Daft with `pip install getdaft`.

## Documentation

[Learn more about Daft in our documentation](https://www.getdaft.io).

## Community

For questions about Daft, please post in our [community hosted on GitHub Discussions](https://github.com/Eventual-Inc/Daft/discussions). We look forward to meeting you there!

## Why Daft?

Processing Complex Data such as images/audio/pointclouds often requires accelerated compute for geometric or machine learning algorithms, much of which leverages existing tooling from the Python/C++ ecosystem. However, many workloads such as analytics, model training data curation and data processing often also require relational query operations for loading/filtering/joining/aggregations.

Daft marries the two worlds with a Dataframe API, enabling you to run both large analytical queries and powerful Complex Data algorithms from the same interface.

1. **Python-first**: Python and Jupyter notebooks are first-class citizens. Daft handles any Python libraries and datastructures natively - use any Python library such as Numpy, OpenCV and PyTorch for Complex Data processing.

2. **Laptop to Cloud**: Daft is built to run as easily on your laptop for interactive development and on your own [Ray](https://www.ray.io) cluster or [Eventual](https://www.eventualcomputing.com) deployment for terabyte-scale production workloads.

3. **Open Data Formats**: Daft loads from and writes to open data formats such as Apache Parquet and Apache Iceberg. It also supports all major cloud vendors' object storage options, allowing you to easily integrate with your existing storage solutions.

