{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURRENTLY NOT WORKING....needs updating!\n",
    "\n",
    "# Using data from AWS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant amount of Earth System Model (ESM) data is publicly available\n",
    "online, including data from the CESM Large Ensemble, CMIP5, and CMIP6 datasets.\n",
    "For accessing a single file, we can specify the file (typically netcdf or zarr\n",
    "format) and its location and then use fsspec (the \"Filesystem Spec+ python\n",
    "package) and xarray to create a array.dataset. For several files, the intake_esm\n",
    "python module (https://github.com/intake/intake-esm) is particularly nice for\n",
    "obtaining the data and put it into an xarray.dataset.\n",
    "\n",
    "This notebook assumes familiarity with the Tutorial Notebook. It additionally\n",
    "shows how to gather data from an ESM collection, put it into a dataset, and then\n",
    "create simple plots using the data with ldcpy.\n",
    "\n",
    "#### Example Data\n",
    "\n",
    "The example data we use is from the CESM Large Ensemble, member 31. This\n",
    "ensemble data has been lossily compressed and reconstructed as part of a blind\n",
    "evaluation study of lossy data compression in LENS (e.g.,\n",
    "http://www.cesm.ucar.edu/projects/community-projects/LENS/projects/lossy-data-compression.html\n",
    "or https://gmd.copernicus.org/articles/9/4381/2016/).\n",
    "\n",
    "Most of the data from the CESM Large Ensemble Project has been made available on\n",
    "Amazon Web Services (Amazon S3), see\n",
    "http://ncar-aws-www.s3-website-us-west-2.amazonaws.com/CESM_LENS_on_AWS.htm .\n",
    "\n",
    "For comparison purposes, the original (non-compressed) data for Ensemble 31 has\n",
    "recently been made available on Amazon Web Services (Amazon S3) in the\n",
    "\"ncar-cesm-lens-baker-lossy-compression-test\" bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ldcpy root to system path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../../../')\n",
    "\n",
    "# Import ldcpy package\n",
    "# Autoreloads package everytime the package is called, so changes to code will be reflected in the notebook if the above sys.path.insert(...) line is uncommented.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import fsspec\n",
    "import intake\n",
    "import xarray as xr\n",
    "\n",
    "import ldcpy\n",
    "\n",
    "# display the plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# silence warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, specify the filesystem and location of the data. Here we are accessing\n",
    "the original data from CESM-LENS ensemble 31, which is available on Amazon S3 in\n",
    "the store named _\"ncar-cesm-lens-baker-lossy-compression-test\"_ bucket.\n",
    "\n",
    "First we listing all available files (which are timeseries files containing a\n",
    "single variable) for that dataset. Note that unlike in the TutorialNotebook\n",
    "(which used NetCDF files), these files are all zarr format. Both monthly and\n",
    "daily data is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=True)\n",
    "stores = fs.ls(\"ncar-cesm-lens-baker-lossy-compression-test/lens-ens31/\")[1:]\n",
    "stores[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we select the file from the store that we want and open it as an\n",
    "xarray.Dataset using xr.open_zarr(). Here we grab data for the first 2D daily\n",
    "variable, FLNS (net longwave flux at surface, in $W/m^2$), in the list (accessed\n",
    "by it location -- stores[0]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = fs.get_mapper(stores[0])\n",
    "ds_flns = xr.open_zarr(store, consolidated=True)\n",
    "ds_flns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returned an xarray.Dataset.\n",
    "\n",
    "Now let's grab the TMQ (Total vertically integrated precipitatable water) and\n",
    "the TS (surface temperature data) and PRECT (precipitation rate) data from AWS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMQ data\n",
    "store2 = fs.get_mapper(stores[16])\n",
    "ds_tmq = xr.open_zarr(store2, consolidated=True)\n",
    "# TS data\n",
    "store3 = fs.get_mapper(stores[20])\n",
    "ds_ts = xr.open_zarr(store3, consolidated=True)\n",
    "# PRECT data\n",
    "store4 = fs.get_mapper(stores[11])\n",
    "ds_prect = xr.open_zarr(store4, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the original data for FLNS and TMQ and TS and PRECT. Next we want to\n",
    "get the lossy compressed variants to compare with these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using intake_esm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will demonstrate using the intake_esm module to get the lossy variants of\n",
    "the files retrieved above. We can use the intake_esm module to search for and\n",
    "open several files as xarray.Dataset objects. The code below is modified from\n",
    "the intake_esm documentation, available here:\n",
    "https://intake-esm.readthedocs.io/en/latest/?badge=latest#overview.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use ensemble 31 data from the CESM-LENS collection on AWS, which (as\n",
    "explained above) has been subjected to lossy compression. Many catalogs for\n",
    "publicly available datasets are accessible via intake-esm can be found at\n",
    "https://github.com/NCAR/intake-esm-datastore/tree/master/catalogs, including for\n",
    "CESM-LENS. We can open that collection as follows (see here:\n",
    "https://github.com/NCAR/esm-collection-spec/blob/master/collection-spec/collection-spec.md#attribute-object):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_loc = (\n",
    "    \"https://raw.githubusercontent.com/NCAR/cesm-lens-aws/master/intake-catalogs/aws-cesm1-le.json\"\n",
    ")\n",
    "aws_col = intake.open_esm_datastore(aws_loc)\n",
    "aws_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we search for the subset of the collection (dataset and variables) that we\n",
    "are interested in. Let's grab FLNS, TMQ, and TS daily data from the atm\n",
    "component for our comparison (available data in this collection is listed here:\n",
    "http://ncar-aws-www.s3-website-us-west-2.amazonaws.com/CESM_LENS_on_AWS.htm).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want daily data for FLNS, TMQ, and TS and PRECT\n",
    "aws_col_subset = aws_col.search(\n",
    "    component=\"atm\",\n",
    "    frequency=\"daily\",\n",
    "    experiment=\"20C\",\n",
    "    variable=[\"FLNS\", \"TS\", \"TMQ\", \"PRECT\"],\n",
    ")\n",
    "# display header info to verify that we got the right variables\n",
    "aws_col_subset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load matching catalog entries into xarray datasets\n",
    "(https://intake-esm.readthedocs.io/en/latest/api.html#intake_esm.core.esm_datastore.to_dataset_dict).\n",
    "We create a dictionary of datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict = aws_col_subset.to_dataset_dict(\n",
    "    zarr_kwargs={\"consolidated\": True, \"decode_times\": True},\n",
    "    storage_options={\"anon\": True},\n",
    "    cdf_kwargs={\"chunks\": {}, \"decode_times\": False},\n",
    ")\n",
    "dset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataset keys to ensure that what we want is present. Here we only have\n",
    "oneentry in the dictonary as we requested the same time period and output\n",
    "frequency for all variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, put the dataset that we are interested from the dictionary into its own\n",
    "dataset variable. (We want the 20th century daily data -- which is our only\n",
    "option.)\n",
    "\n",
    "Also note from above that there are 40 ensemble members - and we just want\n",
    "ensemble 31 (member_id = 30 as can be seen in the coordinates above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_ds = dset_dict[\"atm.20C.daily\"]\n",
    "aws_ds = aws_ds.isel(member_id=30)\n",
    "aws_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have datasets for the original and the lossy compressed data for FLNS,\n",
    "TMQ, PRECT, and TS, which we can extract into a dataset for each variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the three variables from aws_ds as datasets\n",
    "aws_flns = aws_ds[\"FLNS\"].to_dataset()\n",
    "aws_tmq = aws_ds[\"TMQ\"].to_dataset()\n",
    "aws_ts = aws_ds[\"TS\"].to_dataset()\n",
    "aws_prect = aws_ds[\"PRECT\"].to_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use _ldcpy_ to compare the original data to the lossy compressed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use ldcpy, we need to group the data that we want to compare (like variables)\n",
    "into dataset collections. In the Tutorial Notebook, we used\n",
    "ldcpy.open_datasets() to do this as we needed to get the data from the NetCDF\n",
    "files. Here we already loaded the data from AWS into datasets, so we just need\n",
    "to use ldcpy.collect_datasets() to form collections of the datasets that we want\n",
    "to compare.\n",
    "\n",
    "ldcpy.collect_datasets() requires the following three arguments:\n",
    "\n",
    "- _varnames_ : the variable(s) of interest to combine across files (typically\n",
    "  the timeseries file variable name)\n",
    "- _list_of_ds_ : a list of the xarray datasets\n",
    "- _labels_ : a corresponding list of names (or labels) for each dataset in the\n",
    "  collection\n",
    "\n",
    "Note: This function is a wrapper for xarray.concat(), and any additional\n",
    "key/value pairs passed in as a dictionary are used as arguments to\n",
    "xarray.concat().\n",
    "\n",
    "We will create 4 collections for ldcpy (one each for FLNS, TMQ, PRECT, and TS)\n",
    "and assign labels \"original\" and \"lossy\" to the respective datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLNS collection\n",
    "col_flns = ldcpy.collect_datasets(\"cam-fv\", [\"FLNS\"], [ds_flns, aws_flns], [\"original\", \"lossy\"])\n",
    "col_flns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMQ collection\n",
    "col_tmq = ldcpy.collect_datasets(\"cam-fv\", [\"TMQ\"], [ds_tmq, aws_tmq], [\"original\", \"lossy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ts collection\n",
    "col_ts = ldcpy.collect_datasets(\"cam-fv\", [\"TS\"], [ds_ts, aws_ts], [\"original\", \"lossy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRECT collection\n",
    "col_prect = ldcpy.collect_datasets(\n",
    "    \"cam-fv\", [\"PRECT\"], [ds_prect, aws_prect], [\"original\", \"lossy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our collections, we can do some comparisons. Note that these\n",
    "are large files, so make sure you have sufficient compute/memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series plot of PRECT mean in col_ds 'original' dataset - first 100 daysa\n",
    "ldcpy.plot(\n",
    "    col_prect,\n",
    "    \"PRECT\",\n",
    "    sets=[\"original\", \"lossy\"],\n",
    "    calc=\"mean\",\n",
    "    plot_type=\"time_series\",\n",
    "    start=0,\n",
    "    end=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print statistics about 'original', 'lossy', and diff between the two datasets for TMQ at time slice 365\n",
    "ldcpy.compare_stats(col_tmq.isel(time=365), \"TMQ\", [\"original\", \"lossy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your own catalog for intake-esm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data for FLNS and TMQ and TS and PRECT (from Amazon S3 in the\n",
    "\"ncar-cesm-lens-baker-lossy-compression-test\" bucket) was loaded above using\n",
    "method 1. An alternative would be to create our own catalog for this data for\n",
    "use with intake-esm. To illustrate this, we created a test_catalog.csv and\n",
    "test_collection.json file for this particular simple example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first open our collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_col_loc = \"./collections/test_collection.json\"\n",
    "my_col = intake.open_esm_datastore(my_col_loc)\n",
    "my_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the head() gives us the file names\n",
    "my_col.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load all of these into our dictionary! (So we don't need to do the search\n",
    "to make a subset of variables as above in Method 2.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dset_dict = my_col.to_dataset_dict(\n",
    "    zarr_kwargs={\"consolidated\": True, \"decode_times\": True},\n",
    "    storage_options={\"anon\": True},\n",
    ")\n",
    "my_dset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we again just want the 20th century daily data\n",
    "my_ds = my_dset_dict[\"atm.20C.daily\"]\n",
    "my_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a dataset for each variable as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ts = my_ds[\"TS\"].to_dataset()\n",
    "my_tmq = my_ds[\"TMQ\"].to_dataset()\n",
    "my_prect = my_ds[\"PRECT\"].to_dataset()\n",
    "my_flns = my_ds[\"FLNS\"].to_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can form new collections as before and do comparisons...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
