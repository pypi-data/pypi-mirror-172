# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['nlhappy',
 'nlhappy.algorithms',
 'nlhappy.callbacks',
 'nlhappy.components',
 'nlhappy.configs',
 'nlhappy.datamodules',
 'nlhappy.layers',
 'nlhappy.layers.attention',
 'nlhappy.layers.classifier',
 'nlhappy.layers.embedding',
 'nlhappy.metrics',
 'nlhappy.models',
 'nlhappy.models.entity_extraction',
 'nlhappy.models.event_extraction',
 'nlhappy.models.prompt_relation_extraction',
 'nlhappy.models.prompt_span_extraction',
 'nlhappy.models.relation_extraction',
 'nlhappy.models.span_extraction',
 'nlhappy.models.text_classification',
 'nlhappy.models.text_multi_classification',
 'nlhappy.models.text_pair_classification',
 'nlhappy.models.text_pair_regression',
 'nlhappy.models.token_classification',
 'nlhappy.tricks',
 'nlhappy.utils']

package_data = \
{'': ['*'],
 'nlhappy.configs': ['callbacks/*',
                     'datamodule/*',
                     'log_dir/*',
                     'logger/*',
                     'model/*',
                     'trainer/*']}

install_requires = \
['datasets>=2.0.0',
 'googletrans==4.0.0rc1',
 'hydra-colorlog>=1.1.0',
 'hydra-core==1.1',
 'pytorch-lightning==1.7.7',
 'rich>=12.4.3,<13.0.0',
 'torch>=1.11.0',
 'transformers>=4.17.0']

extras_require = \
{':extra == "spacy" or extra == "all"': ['spacy>=3.3.0'],
 ':extra == "wandb" or extra == "all"': ['wandb>=0.12.18']}

entry_points = \
{'console_scripts': ['nlhappy = nlhappy.run:train'],
 'spacy_factories': ['span_classifier = nlhappy.components:get_spancat']}

setup_kwargs = {
    'name': 'nlhappy',
    'version': '2022.10.20',
    'description': 'è‡ªç„¶è¯­è¨€å¤„ç†(NLP)',
    'long_description': '\n<div align=\'center\'>\n\n# NLHappy\n<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>\n<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>\n<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>\n<a href="https://github.com/ashleve/lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a>\n<a href="https://spacy.io/"><img alt="Spacy" src="https://img.shields.io/badge/component-%20Spacy-blue"></a>\n<a href="https://wandb.ai/"><img alt="WanDB" src="https://img.shields.io/badge/Log-WanDB-brightgreen"></a>\n</div>\n<br><br>\n\n## ğŸ“Œ&nbsp;&nbsp; ç®€ä»‹\n\nnlhappyæ˜¯ä¸€æ¬¾é›†æˆäº†æ•°æ®å¤„ç†,æ¨¡å‹è®­ç»ƒ,æ–‡æœ¬å¤„ç†æµç¨‹æ„å»ºç­‰å„ç§åŠŸèƒ½çš„è‡ªç„¶è¯­è¨€å¤„ç†åº“,å¹¶ä¸”å†…ç½®äº†å„ç§ä»»åŠ¡çš„SOTAæ–¹æ¡ˆ,ç›¸ä¿¡é€šè¿‡nlhappyå¯ä»¥è®©ä½ æ›´æ„‰æ‚¦çš„åšå„ç§nlpä»»åŠ¡\n> å®ƒä¸»è¦çš„ä¾èµ–æœ‰\n- [pytorch-lightning](https://pytorch-lightning.readthedocs.io/en/latest/): ç”¨äºæ¨¡å‹çš„è®­ç»ƒ\n- [transformers](https://huggingface.co/docs/transformers/index): è·å–é¢„è®­ç»ƒæ¨¡å‹\n- [datasets](https://huggingface.co/docs/datasets/index): æ„å»ºæ•°æ®é›†\n- [spacy](https://spacy.io/usage): ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†æµç¨‹å’Œç»„ä»¶æ„å»º\n\n\n## ğŸš€&nbsp;&nbsp; å®‰è£…\n<details>\n<summary><b>å®‰è£…nlhappy</b></summary>\n\n> æ¨èå…ˆå»[pytorchå®˜ç½‘](https://pytorch.org/get-started/locally/)å®‰è£…pytorchå’Œå¯¹åº”cuda\n```bash\n# pip å®‰è£…\npip install --upgrade pip\npip install --upgrade nlhappy\n```\n</details>\n\n<details>\n<summary><b>å…¶ä»–å¯é€‰</b></summary>\n\n> æ¨èå®‰è£…wandbç”¨äºå¯è§†åŒ–è®­ç»ƒæ—¥å¿—\n- æ³¨å†Œ: https://wandb.ai/\n- è·å–è®¤è¯: https://wandb.ai/authorize\n- ç™»é™†:\n```bash\nwandb login\n```\næ¨¡å‹è®­ç»ƒå¼€å§‹åå»[å®˜ç½‘](https://wandb.ai/)æŸ¥çœ‹è®­ç»ƒå®å†µ\n</details>\n\n\n\n\n## âš¡&nbsp;&nbsp; å¿«é€Ÿå¼€å§‹\n\n> ä»»åŠ¡ç¤ºä¾‹çš„æ•°æ®é›†è·å–:[CBLUEå®˜ç½‘](https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414)\n\n<details>\n<summary><b>æ–‡æœ¬åˆ†ç±»</b></summary>\n\n> åˆ¶ä½œæ•°æ®é›†\n```python\nfrom nlhappy.datamodules import TextClassificationDataModule\nfrom nlhappy.utils.make_dataset import DatasetDict, Dataset\nimport srsly\n\n# CBLUEçŸ­æ–‡æœ¬åˆ†ç±»æ•°æ®é›†CHIP-CTCä¸ºä¾‹\n\n# æŸ¥çœ‹æ•°æ®é›†æ ¼å¼\nexample = TextClassificationDataModule.show_one_example()\n# example : {\'label\': \'æ–°é—»\', \'text\': \'æ€ä¹ˆç»™è¿™ä¸ªå›¾ç‰‡æ·»åŠ è¶…çº§é“¾æ¥å‘¢ï¼Ÿ\'}\n\n# åˆ¶ä½œæ•°æ®é›†\ntrain_data = list(srsly.read_json(\'assets/CHIP-CTC/CHIP-CTC_train.json\'))\nval_data = list(srsly.read_json(\'assets/CHIP-CTC/CHIP-CTC_dev.json\'))\ndef convert_to_dataset(data):\n    dataset = {\'text\':[], \'label\':[]}\n    for d in data:\n        dataset[\'text\'].append(d[\'text\'])\n        dataset[\'label\'].append(d[\'label\'])\n    return Dataset.from_dict(dataset)\ntrain_ds = convert_to_dataset(train_data)\nval_ds = convert_to_dataset(val_data)\ndataset_dict = DatasetDict({\'train\':train_ds, \'validation\':val_ds})\n\n# ä¿å­˜æ•°æ®é›†, datasetsä¸ºnlhappyé»˜è®¤æ•°æ®é›†è·¯å¾„\ndataset_dict.save_to_disk(\'./datasets/CHIP-CTC\')\n```\n> è®­ç»ƒæ¨¡å‹\n\n- å‘½ä»¤è¡Œæˆ–bashè„šæœ¬,æ–¹ä¾¿å¿«æ·\n```\nnlhappy \\\ndatamodule=text_classification \\\ndatamodule.dataset=TNEWS \\\n# huffingfaceçš„é¢„è®­ç»ƒæ¨¡å‹éƒ½ä¼šæ”¯æŒ\ndatamodule.plm=hfl/chinese-roberta-wwm-ext \\ \ndatamodule.batch_size=32 \\\nmodel=bert_tc \\\nmodel.lr=3e-5 \\\nseed=1234 \\\ntrainer=default\n# é»˜è®¤ä¸ºå•gpu 0å·æ˜¾å¡è®­ç»ƒ,å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿®æ”¹æ˜¾å¡\n# trainer.devices=[1]\n# å•å¡åŠç²¾åº¦è®­ç»ƒ\n# trainer.precision=16\n# ä½¿ç”¨wandbè®°å½•æ—¥å¿—\n# logger=wandb\n# å¤šå¡è®­ç»ƒ\n# trainer=ddp trainer.devices=4\n```\n\n> æ¨¡å‹é¢„æµ‹\n```python\nfrom nlhappy.models import BertTextClassification\n# åŠ è½½è®­ç»ƒçš„checkpoint\nckpt = \'logs/path/***.ckpt\'\nmodel = BertTextClassification.load_from_ckeckpoint(ckpt)\ntext = \'ç ”ç©¶å¼€å§‹å‰30å¤©å†…ï¼Œæ¥å—è¿‡å…¶ä»–ä¸´åºŠæ–¹æ¡ˆæ²»ç–—ï¼›\'\nscores = model.predict(text=text, device=\'cpu\')\n# è½¬ä¸ºonnxæ¨¡å‹\nmodel.to_onnx(\'path/tc.onnx\')\nmodel.tokenizer.save_pretrained(\'path/tokenizer\')\n```\n</details>\n\n<details>\n<summary><b>å®ä½“æŠ½å–</b></summary>\n\nnlhappyæ”¯æŒæ­£å¸¸,åµŒå¥—å’Œéè¿ç»­çš„å®ä½“æŠ½å–ä»»åŠ¡,ä¸‹é¢ä»¥å¯ä»¥è§£å†³åµŒå¥—ä»»åŠ¡çš„æ¨¡å‹globalpointerä¸ºä¾‹\n> åˆ¶ä½œæ•°æ®é›†\n```python\n# CBLUEå®ä½“è¯†åˆ«æ•°æ®é›†CMeEE\nfrom nlhappy.datamodules import EntityExtractionDataModule\nfrom nlhappy.models.entity_extraction import GlobalPointerForEntityExtraction\nfrom nlhappy.utils.make_dataset import Dataset, DatasetDict\nimport srsly\n\n#æŸ¥çœ‹æ•°æ®é›†æ ¼å¼\nexample = EntityExtractionDataModule.show_one_example()\n# example : {"text":"è¿™æ˜¯ä¸€ä¸ªé•¿é¢ˆé¹¿","entities":[{"indexes":[4,5,6],"label":"åŠ¨ç‰©", "text":"é•¿é¢ˆé¹¿"}]}\n\n# æ ¹æ®æ ¼å¼åˆ¶ä½œæ•°æ®é›†\ntrain_data = list(srsly.read_json(\'assets/CMeEE/CMeEE_train.json\'))\nval_data = list(srsly.read_json(\'assets/CMeEE/CMeEE_dev.json\'))\ndef convert_to_dataset(data):\n    ds = {\'text\':[],\'entities\':[]}\n    for d in data:\n        ents = []\n        ds[\'text\'].append(d[\'text\'])\n        for e in d[\'entities\']:\n            if len(e[\'entity\'])>0:\n                ent = {}\n                ent[\'text\'] = e[\'entity\']\n                ent[\'indexes\'] = [idx for idx in range(len(e[\'entity\']))]\n                ent[\'text\'] = e[\'entity\']\n                ent[\'label\'] = e[\'type\']\n                ents.append(ent)\n        ds[\'entities\'].append(ents)\n    return Dataset.from_dict(ds) \ntrain_ds = convert_to_dataset(train_data)\nval_ds = convert_to_dataset(val_data)\ndataset_dict = DatasetDict({\'train\':train_ds, \'validation\':val_ds})\ndataset_dict.save_to_disk(\'./datasets/CMeEE\')\n\n\n```\n> è®­ç»ƒæ¨¡å‹\n\n- ç¼–å†™è®­ç»ƒè„šæœ¬\n```bash\nnlhappy \\\ndatamodule=entity_extraction \\\ndatamodule.dataset=CMeEE \\\ndatamodule.plm=hfl/chinese-roberta-wwm-ext \\\ndatamodule.batch_size=16 \\\nmodel=globalpointer \\\nmodel.lr=3e-5 \\\nseed=123\n# é»˜è®¤trainerä¸ºå•å¡è®­ç»ƒ\ntrainer=default\n\n# å¤šå¡\n# trainer=ddp \\\n# trainer.devices=4 \\\n# trainer.precision=16 #åŠç²¾åº¦\n\n# å¦‚æœå®‰è£…äº†wandbåˆ™å¯ä»¥ä½¿ç”¨wandb\n# logger=wandb \n```\n\n> æ¨¡å‹æ¨ç†\n\n```python\nfrom nlhappy.models import GlobalPointer\nckpt = \'logs/path/***.ckpt\'\nmodel = GlobalPointer.load_from_ckeckpoint(ckpt)\nents = model.predcit(\'æ–‡æœ¬\')\n\n# è½¬ä¸ºonnxæ¨¡å‹,è¿›è¡Œåç»­éƒ¨ç½²\nmodel.to_onnx(\'path/tc.onnx\')\nmodel.tokenizer.save_pretrained(\'path/tokenizer\')\n```\n</details>\n\n<details>\n<summary><b>å…³ç³»æŠ½å–</b></summary>\nTODO\n</details>\n\n<details>\n<summary><b>äº‹ä»¶æŠ½å–</b></summary>\nTODO\n</details>\n\n<details>\n<summary><b>æ‘˜è¦</b></summary>\nTODO\n</details>\n\n<details>\n<summary><b>ç¿»è¯‘</b></summary>\nTODO\n</details>\n\n\n## è®ºæ–‡å¤ç°',
    'author': 'wangmengdi',
    'author_email': '790990241@qq.om',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'extras_require': extras_require,
    'entry_points': entry_points,
    'python_requires': '>=3.7,<4.0',
}


setup(**setup_kwargs)
