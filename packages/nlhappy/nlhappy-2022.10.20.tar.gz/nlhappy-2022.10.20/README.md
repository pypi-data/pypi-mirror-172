
<div align='center'>

# NLHappy
<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>
<a href="https://github.com/ashleve/lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a>
<a href="https://spacy.io/"><img alt="Spacy" src="https://img.shields.io/badge/component-%20Spacy-blue"></a>
<a href="https://wandb.ai/"><img alt="WanDB" src="https://img.shields.io/badge/Log-WanDB-brightgreen"></a>
</div>
<br><br>

## ğŸ“Œ&nbsp;&nbsp; ç®€ä»‹

nlhappyæ˜¯ä¸€æ¬¾é›†æˆäº†æ•°æ®å¤„ç†,æ¨¡å‹è®­ç»ƒ,æ–‡æœ¬å¤„ç†æµç¨‹æ„å»ºç­‰å„ç§åŠŸèƒ½çš„è‡ªç„¶è¯­è¨€å¤„ç†åº“,å¹¶ä¸”å†…ç½®äº†å„ç§ä»»åŠ¡çš„SOTAæ–¹æ¡ˆ,ç›¸ä¿¡é€šè¿‡nlhappyå¯ä»¥è®©ä½ æ›´æ„‰æ‚¦çš„åšå„ç§nlpä»»åŠ¡
> å®ƒä¸»è¦çš„ä¾èµ–æœ‰
- [pytorch-lightning](https://pytorch-lightning.readthedocs.io/en/latest/): ç”¨äºæ¨¡å‹çš„è®­ç»ƒ
- [transformers](https://huggingface.co/docs/transformers/index): è·å–é¢„è®­ç»ƒæ¨¡å‹
- [datasets](https://huggingface.co/docs/datasets/index): æ„å»ºæ•°æ®é›†
- [spacy](https://spacy.io/usage): ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†æµç¨‹å’Œç»„ä»¶æ„å»º


## ğŸš€&nbsp;&nbsp; å®‰è£…
<details>
<summary><b>å®‰è£…nlhappy</b></summary>

> æ¨èå…ˆå»[pytorchå®˜ç½‘](https://pytorch.org/get-started/locally/)å®‰è£…pytorchå’Œå¯¹åº”cuda
```bash
# pip å®‰è£…
pip install --upgrade pip
pip install --upgrade nlhappy
```
</details>

<details>
<summary><b>å…¶ä»–å¯é€‰</b></summary>

> æ¨èå®‰è£…wandbç”¨äºå¯è§†åŒ–è®­ç»ƒæ—¥å¿—
- æ³¨å†Œ: https://wandb.ai/
- è·å–è®¤è¯: https://wandb.ai/authorize
- ç™»é™†:
```bash
wandb login
```
æ¨¡å‹è®­ç»ƒå¼€å§‹åå»[å®˜ç½‘](https://wandb.ai/)æŸ¥çœ‹è®­ç»ƒå®å†µ
</details>




## âš¡&nbsp;&nbsp; å¿«é€Ÿå¼€å§‹

> ä»»åŠ¡ç¤ºä¾‹çš„æ•°æ®é›†è·å–:[CBLUEå®˜ç½‘](https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414)

<details>
<summary><b>æ–‡æœ¬åˆ†ç±»</b></summary>

> åˆ¶ä½œæ•°æ®é›†
```python
from nlhappy.datamodules import TextClassificationDataModule
from nlhappy.utils.make_dataset import DatasetDict, Dataset
import srsly

# CBLUEçŸ­æ–‡æœ¬åˆ†ç±»æ•°æ®é›†CHIP-CTCä¸ºä¾‹

# æŸ¥çœ‹æ•°æ®é›†æ ¼å¼
example = TextClassificationDataModule.show_one_example()
# example : {'label': 'æ–°é—»', 'text': 'æ€ä¹ˆç»™è¿™ä¸ªå›¾ç‰‡æ·»åŠ è¶…çº§é“¾æ¥å‘¢ï¼Ÿ'}

# åˆ¶ä½œæ•°æ®é›†
train_data = list(srsly.read_json('assets/CHIP-CTC/CHIP-CTC_train.json'))
val_data = list(srsly.read_json('assets/CHIP-CTC/CHIP-CTC_dev.json'))
def convert_to_dataset(data):
    dataset = {'text':[], 'label':[]}
    for d in data:
        dataset['text'].append(d['text'])
        dataset['label'].append(d['label'])
    return Dataset.from_dict(dataset)
train_ds = convert_to_dataset(train_data)
val_ds = convert_to_dataset(val_data)
dataset_dict = DatasetDict({'train':train_ds, 'validation':val_ds})

# ä¿å­˜æ•°æ®é›†, datasetsä¸ºnlhappyé»˜è®¤æ•°æ®é›†è·¯å¾„
dataset_dict.save_to_disk('./datasets/CHIP-CTC')
```
> è®­ç»ƒæ¨¡å‹

- å‘½ä»¤è¡Œæˆ–bashè„šæœ¬,æ–¹ä¾¿å¿«æ·
```
nlhappy \
datamodule=text_classification \
datamodule.dataset=TNEWS \
# huffingfaceçš„é¢„è®­ç»ƒæ¨¡å‹éƒ½ä¼šæ”¯æŒ
datamodule.plm=hfl/chinese-roberta-wwm-ext \ 
datamodule.batch_size=32 \
model=bert_tc \
model.lr=3e-5 \
seed=1234 \
trainer=default
# é»˜è®¤ä¸ºå•gpu 0å·æ˜¾å¡è®­ç»ƒ,å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿®æ”¹æ˜¾å¡
# trainer.devices=[1]
# å•å¡åŠç²¾åº¦è®­ç»ƒ
# trainer.precision=16
# ä½¿ç”¨wandbè®°å½•æ—¥å¿—
# logger=wandb
# å¤šå¡è®­ç»ƒ
# trainer=ddp trainer.devices=4
```

> æ¨¡å‹é¢„æµ‹
```python
from nlhappy.models import BertTextClassification
# åŠ è½½è®­ç»ƒçš„checkpoint
ckpt = 'logs/path/***.ckpt'
model = BertTextClassification.load_from_ckeckpoint(ckpt)
text = 'ç ”ç©¶å¼€å§‹å‰30å¤©å†…ï¼Œæ¥å—è¿‡å…¶ä»–ä¸´åºŠæ–¹æ¡ˆæ²»ç–—ï¼›'
scores = model.predict(text=text, device='cpu')
# è½¬ä¸ºonnxæ¨¡å‹
model.to_onnx('path/tc.onnx')
model.tokenizer.save_pretrained('path/tokenizer')
```
</details>

<details>
<summary><b>å®ä½“æŠ½å–</b></summary>

nlhappyæ”¯æŒæ­£å¸¸,åµŒå¥—å’Œéè¿ç»­çš„å®ä½“æŠ½å–ä»»åŠ¡,ä¸‹é¢ä»¥å¯ä»¥è§£å†³åµŒå¥—ä»»åŠ¡çš„æ¨¡å‹globalpointerä¸ºä¾‹
> åˆ¶ä½œæ•°æ®é›†
```python
# CBLUEå®ä½“è¯†åˆ«æ•°æ®é›†CMeEE
from nlhappy.datamodules import EntityExtractionDataModule
from nlhappy.models.entity_extraction import GlobalPointerForEntityExtraction
from nlhappy.utils.make_dataset import Dataset, DatasetDict
import srsly

#æŸ¥çœ‹æ•°æ®é›†æ ¼å¼
example = EntityExtractionDataModule.show_one_example()
# example : {"text":"è¿™æ˜¯ä¸€ä¸ªé•¿é¢ˆé¹¿","entities":[{"indexes":[4,5,6],"label":"åŠ¨ç‰©", "text":"é•¿é¢ˆé¹¿"}]}

# æ ¹æ®æ ¼å¼åˆ¶ä½œæ•°æ®é›†
train_data = list(srsly.read_json('assets/CMeEE/CMeEE_train.json'))
val_data = list(srsly.read_json('assets/CMeEE/CMeEE_dev.json'))
def convert_to_dataset(data):
    ds = {'text':[],'entities':[]}
    for d in data:
        ents = []
        ds['text'].append(d['text'])
        for e in d['entities']:
            if len(e['entity'])>0:
                ent = {}
                ent['text'] = e['entity']
                ent['indexes'] = [idx for idx in range(len(e['entity']))]
                ent['text'] = e['entity']
                ent['label'] = e['type']
                ents.append(ent)
        ds['entities'].append(ents)
    return Dataset.from_dict(ds) 
train_ds = convert_to_dataset(train_data)
val_ds = convert_to_dataset(val_data)
dataset_dict = DatasetDict({'train':train_ds, 'validation':val_ds})
dataset_dict.save_to_disk('./datasets/CMeEE')


```
> è®­ç»ƒæ¨¡å‹

- ç¼–å†™è®­ç»ƒè„šæœ¬
```bash
nlhappy \
datamodule=entity_extraction \
datamodule.dataset=CMeEE \
datamodule.plm=hfl/chinese-roberta-wwm-ext \
datamodule.batch_size=16 \
model=globalpointer \
model.lr=3e-5 \
seed=123
# é»˜è®¤trainerä¸ºå•å¡è®­ç»ƒ
trainer=default

# å¤šå¡
# trainer=ddp \
# trainer.devices=4 \
# trainer.precision=16 #åŠç²¾åº¦

# å¦‚æœå®‰è£…äº†wandbåˆ™å¯ä»¥ä½¿ç”¨wandb
# logger=wandb 
```

> æ¨¡å‹æ¨ç†

```python
from nlhappy.models import GlobalPointer
ckpt = 'logs/path/***.ckpt'
model = GlobalPointer.load_from_ckeckpoint(ckpt)
ents = model.predcit('æ–‡æœ¬')

# è½¬ä¸ºonnxæ¨¡å‹,è¿›è¡Œåç»­éƒ¨ç½²
model.to_onnx('path/tc.onnx')
model.tokenizer.save_pretrained('path/tokenizer')
```
</details>

<details>
<summary><b>å…³ç³»æŠ½å–</b></summary>
TODO
</details>

<details>
<summary><b>äº‹ä»¶æŠ½å–</b></summary>
TODO
</details>

<details>
<summary><b>æ‘˜è¦</b></summary>
TODO
</details>

<details>
<summary><b>ç¿»è¯‘</b></summary>
TODO
</details>


## è®ºæ–‡å¤ç°